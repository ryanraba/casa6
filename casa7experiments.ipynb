{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "casa7experiments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanraba/casa6/blob/master/casa7experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xAxbeSkCwg6",
        "colab_type": "text"
      },
      "source": [
        "# Setup libraries, tools, data etc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50o-JqkvXTTS",
        "colab_type": "code",
        "outputId": "de846eac-cc24-47d2-fadc-f7b85c3497bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "print(\"installing pre-requisite packages...\")\n",
        "#os.system(\"pip install pycuda\")\n",
        "os.system(\"pip install pyarrow\")\n",
        "#os.system(\"pip install scikit-cuda\")\n",
        "os.system(\"apt-get install libgfortran3\")\n",
        "\n",
        "print(\"installing casatasks...\")\n",
        "os.system(\"pip install --extra-index-url https://casa-pip.nrao.edu:443/repository/pypi-group/simple casatools\")\n",
        "os.system(\"pip install --extra-index-url https://casa-pip.nrao.edu:443/repository/pypi-group/simple casatasks\")\n",
        "\n",
        "print(\"downloading MeasurementSet from CASAguide First Look at Imaging\")\n",
        "os.system(\"wget https://bulk.cv.nrao.edu/almadata/public/working/sis14_twhya_calibrated_flagged.ms.tar\")\n",
        "os.system(\"tar -xvf sis14_twhya_calibrated_flagged.ms.tar\")\n",
        "\n",
        "print('complete')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing pre-requisite packages...\n",
            "installing casatasks...\n",
            "downloading MeasurementSet from CASAguide First Look at Imaging\n",
            "complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpfmTWxySLWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep this updated if dataset changes\n",
        "prefix = 'sis14_twhya_calibrated_flagged'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WoxxqRWENYa",
        "colab_type": "text"
      },
      "source": [
        "# Convert MS to HDF5 and Apache Parquet formats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fftCt81Yr3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "36660afd-eded-4796-d3cf-08fe73208aec"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from casatasks import exportuvfits\n",
        "from astropy.io import fits\n",
        "\n",
        "print('converting MS to FITS...')\n",
        "exportuvfits(vis=prefix+'.ms', fitsfile=prefix+'.fits', overwrite=True)\n",
        "\n",
        "print('processing FITS...')\n",
        "fhdr = fits.getheader(prefix+'.fits')\n",
        "fdata = fits.getdata(prefix+'.fits')\n",
        "\n",
        "# make visibility data frame\n",
        "# flatten channels to 2-D table\n",
        "vd = fdata.data[:,0,0,0,:,:,:2].reshape(len(fdata),-1).astype(np.float32)\n",
        "labels = np.concatenate([['p0r'+str(ii), 'p0i'+str(ii), 'p1r'+str(ii), 'p1i'+str(ii)] for ii in range(vd.shape[1]//4)])\n",
        "vdf = pd.DataFrame(data=vd, columns=labels)\n",
        "\n",
        "# put everything else together\n",
        "md = {}\n",
        "for par in np.unique(fdata.parnames):\n",
        "  md[par] = fdata.par(par)\n",
        "\n",
        "mdf = pd.DataFrame(data=md).merge(vdf, left_index=True, right_index=True)\n",
        "\n",
        "# save dataframe to hdf5\n",
        "print('saving FITS to HDF5...')\n",
        "mdf.to_hdf(prefix+'.h5', key='mdf', mode='w')\n",
        "\n",
        "# save to parquet\n",
        "# this time, don't flatten to 2-D, keep visibilities as np.arrays\n",
        "print('saving FITS to Parquet...')\n",
        "vd = np.swapaxes(fdata.data[:,0,0,0,:,:,:2].reshape(len(fdata),-1,4).astype(np.float32), 2, 1)\n",
        "labels = ['p0real', 'p0imag', 'p1real', 'p1imag']\n",
        "\n",
        "# there must be a better way to do this, but I can't think of it right now\n",
        "tt = np.empty(len(vd)*4,object).reshape(-1,4)\n",
        "for ii in range(len(vd)):\n",
        "  for jj in range(4):\n",
        "    tt[ii,jj] = vd[ii,jj,:]\n",
        "vdf = pd.DataFrame(data=tt, columns=labels)\n",
        "mdf = pd.DataFrame(data=md).merge(vdf, left_index=True, right_index=True)\n",
        "\n",
        "pq.write_to_dataset(pa.Table.from_pandas(mdf), \n",
        "                    root_path=prefix+'.parquet',\n",
        "                    flavor='spark',\n",
        "                    partition_cols=['BASELINE'])\n",
        "\n",
        "print('complete')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converting MS to FITS...\n",
            "processing FITS...\n",
            "saving FITS to HDF5...\n",
            "saving FITS to Parquet...\n",
            "complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V39hpsHPMDZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleanup some memory\n",
        "fhdr, fdata, vd, md, vdf, mdf, tt = [[], [], [], [], [], [], []]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCWV-EEMK34z",
        "colab_type": "text"
      },
      "source": [
        "# CPU Processing with Pandas/Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG7NRcepTc7-",
        "colab_type": "code",
        "outputId": "b039d677-dfd1-44ff-9183-3abef0c2fa68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.signal as sp\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# load pandas dataframe\n",
        "pdf = pd.read_hdf(prefix+'.h5', key='mdf')\n",
        "\n",
        "# note that visibilities are stored as a flat table with column name pxyz \n",
        "# where x = polarization(?), y = r (real) or i (imaginary), and z = channel\n",
        "\n",
        "r0chans = pdf.columns.values[pdf.columns.str.startswith('p0r')]\n",
        "i0chans = pdf.columns.values[pdf.columns.str.startswith('p0i')]\n",
        "r1chans = pdf.columns.values[pdf.columns.str.startswith('p1r')]\n",
        "i1chans = pdf.columns.values[pdf.columns.str.startswith('p1i')]\n",
        "\n",
        "# start the race\n",
        "start = time.time()\n",
        "\n",
        "################################################################\n",
        "\n",
        "# I don't really know what we do with the second set of points, so lets just average in to the first\n",
        "# let's also use complex numbers whenever the functions let us\n",
        "c0 = pdf[r0chans].values + 1j*pdf[i0chans].values  # first set of complex visibilities\n",
        "c1 = pdf[r1chans].values + 1j*pdf[i1chans].values  # second set of complex visibilities\n",
        "vis = (c0 + c1)/2\n",
        "\n",
        "# lets do some math to take up time\n",
        "CONT_RMS = np.sqrt(np.mean(np.power(np.abs(vis),2), axis=1))\n",
        "CONT_MAX = np.max(np.abs(vis), axis=1)\n",
        "CONT_STD = np.std(np.abs(vis), axis=1)\n",
        "\n",
        "################################################################\n",
        "print('pandas elapsed time (s): ', time.time() - start)\n",
        "\n",
        "print(vis[99,:10])\n",
        "print(CONT_RMS)\n",
        "print(CONT_MAX)\n",
        "print(CONT_STD)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pandas elapsed time (s):  2.1277143955230713\n",
            "[13.544712  -4.1790752j  11.385413  -1.468117j    6.5120544 -0.63292366j\n",
            "  4.2294087 -8.978855j   16.402962  -3.087995j    7.351376  -1.7305609j\n",
            "  4.109383  -3.5484805j  10.181942  -1.7112226j   9.926709  -4.0611053j\n",
            "  4.697096 -13.006346j  ]\n",
            "[10.410056 10.482974 10.374875 ... 10.956581 10.379412  8.499947]\n",
            "[25.624918 21.068535 21.712814 ... 29.41013  27.530628 27.838688]\n",
            "[4.1418676 4.1568103 3.8755553 ... 5.051679  4.639209  4.07304  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zljCL8jMvWY",
        "colab_type": "text"
      },
      "source": [
        "#  CPU Processing with Apache Spark\n",
        "https://spark.apache.org/docs/latest/api/python/index.html\n",
        "\n",
        "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDq9WAdhCngg",
        "colab_type": "code",
        "outputId": "d24f8bfa-3cc0-4082-b854-663040b37b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# install and configure PySpark\n",
        "import os\n",
        "print(\"installing spark...\")\n",
        "os.system(\"apt-get install openjdk-8-jdk-headless -qq\")\n",
        "os.system(\"pip install findspark\")\n",
        "os.system(\"wget https://www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\")\n",
        "os.system(\"tar -xvzf spark-2.4.3-bin-hadoop2.7.tgz\")\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\"\n",
        "print('complete') "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing spark...\n",
            "complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axx4SEqqiRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c89d3020-c039-4568-e643-7da26b124e77"
      },
      "source": [
        "print('initializing spark...') \n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.config('spark.driver.memory', '6g')\n",
        "spark = spark.config('spark.executor.memory', '4g')\n",
        "spark = spark.config('spark.executor.cores', '4')\n",
        "spark = spark.master(\"local[*]\").getOrCreate()\n",
        "spark.sparkContext._conf.getAll() "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initializing spark...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.host', '47dc05493f45'),\n",
              " ('spark.driver.memory', '6g'),\n",
              " ('spark.app.id', 'local-1559179842047'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.executor.memory', '4g'),\n",
              " ('spark.driver.port', '45713'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.executor.cores', '4')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xohcbc8NsCa",
        "colab_type": "code",
        "outputId": "b20da285-f76c-4eab-a07a-467d0506f96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# load spark dataframe\n",
        "sdf = spark.read.parquet(prefix+'.parquet')\n",
        "sdf.cache()\n",
        "sdf.count() # trigger a load to cache data in memory\n",
        "\n",
        "# start the race\n",
        "start = time.time()\n",
        "\n",
        "################################################################\n",
        "\n",
        "# here are our map operations to be run on each row independently\n",
        "def processElement(element):  \n",
        "  c0 = (np.array(element.p0real) + 1j*np.array(element.p0imag)).astype(np.complex64)\n",
        "  c1 = (np.array(element.p1real) + 1j*np.array(element.p1imag)).astype(np.complex64)\n",
        "  vis = (c0 + c1)/2\n",
        "  \n",
        "  CONT_RMS = np.sqrt(np.mean(np.power(np.abs(vis),2), axis=0))\n",
        "  CONT_MAX = np.max(np.abs(vis), axis=0)\n",
        "  CONT_STD = np.std(np.abs(vis), axis=0)\n",
        "  \n",
        "  return vis, CONT_RMS, CONT_MAX, CONT_STD\n",
        "  \n",
        "# fire off our map function\n",
        "prdd = sdf.rdd.map(lambda row: processElement(row)).collect()\n",
        "\n",
        "################################################################\n",
        "print('spark elapsed time (s): ', time.time() - start)\n",
        "\n",
        "print(np.array(prdd)[99][0][:10])\n",
        "print(np.array(prdd)[:,1])\n",
        "print(np.array(prdd)[:,2])\n",
        "print(np.array(prdd)[:,3])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark elapsed time (s):  33.393213510513306\n",
            "[15.368347  +3.496803j   2.352149  +6.232252j  -2.8458345 -3.4297054j\n",
            " -0.3756826 -4.024106j   2.6655807 -5.9877825j  0.42628008-4.511368j\n",
            "  3.692929  +4.503709j   7.998578  +6.80622j   11.813255  +4.1906047j\n",
            " -1.5166845 +6.9134226j]\n",
            "[11.02426 10.730138 10.810746 ... 12.037731 11.975934 12.026075]\n",
            "[24.063694 23.316742 23.461426 ... 29.285095 26.563631 29.69375]\n",
            "[4.315754 4.5817556 4.179552 ... 5.626123 5.6413765 5.4935527]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txYoU79l_o-a",
        "colab_type": "text"
      },
      "source": [
        "# GPU Processing with Tensorflow\n",
        "https://www.tensorflow.org/api_docs/python/tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwgEHrxYy16h",
        "colab_type": "code",
        "outputId": "ec16f5c5-24cb-47d7-90c9-061317f6fe6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# load pandas dataframe\n",
        "pdf = pd.read_hdf(prefix+'.h5', key='mdf')\n",
        "\n",
        "# note that visibilities are stored as a flat table with column name pxyz \n",
        "# where x = polarization(?), y = r (real) or i (imaginary), and z = channel\n",
        "\n",
        "r0chans = pdf.columns.values[pdf.columns.str.startswith('p0r')]\n",
        "i0chans = pdf.columns.values[pdf.columns.str.startswith('p0i')]\n",
        "r1chans = pdf.columns.values[pdf.columns.str.startswith('p1r')]\n",
        "i1chans = pdf.columns.values[pdf.columns.str.startswith('p1i')]\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "# create our tensor math sequence\n",
        "with tf.device('/gpu:0'):\n",
        "  c0 = pdf[r0chans].values + 1j*pdf[i0chans].values  # first set of complex visibilities\n",
        "  c1 = pdf[r1chans].values + 1j*pdf[i1chans].values  # second set of complex visibilities\n",
        "  \n",
        "  aa = tf.placeholder(tf.complex64, shape=c0.shape)\n",
        "  bb = tf.placeholder(tf.complex64, shape=c1.shape)\n",
        "  tvis = tf.scalar_mul(0.5, tf.add(aa, bb))\n",
        "  \n",
        "  # lets do some math to take up time\n",
        "  tCONT_RMS = tf.sqrt( tf.reduce_mean( tf.square( tf.abs(tvis) ), axis=1) )          \n",
        "  tCONT_MAX = tf.reduce_max( tf.abs(tvis), axis=1)\n",
        "  tCONT_STD = tf.math.reduce_std( tf.abs(tvis), axis=1)\n",
        "\n",
        "with tf.Session() as sess:    \n",
        "    vis = sess.run(tvis, feed_dict={aa: c0, bb: c1})\n",
        "    CONT_RMS = sess.run(tCONT_RMS, feed_dict={aa: c0, bb: c1})\n",
        "    CONT_MAX = sess.run(tCONT_MAX, feed_dict={aa: c0, bb: c1})\n",
        "    CONT_STD = sess.run(tCONT_STD, feed_dict={aa: c0, bb: c1})\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "print('tensorflow elapsed time (s): ', time.time() - start)\n",
        "\n",
        "print(vis[99,:10])\n",
        "print(CONT_RMS)\n",
        "print(CONT_MAX)\n",
        "print(CONT_STD)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow elapsed time (s):  1.1716275215148926\n",
            "[13.544712  -4.1790752j  11.385413  -1.468117j    6.5120544 -0.63292366j\n",
            "  4.2294087 -8.978855j   16.402962  -3.087995j    7.351376  -1.7305609j\n",
            "  4.109383  -3.5484805j  10.181942  -1.7112226j   9.926709  -4.0611053j\n",
            "  4.697096 -13.006346j  ]\n",
            "[10.410055 10.482974 10.374873 ... 10.956581 10.379413  8.499947]\n",
            "[25.624916 21.068535 21.712816 ... 29.41013  27.530628 27.838688]\n",
            "[4.1418676 4.1568103 3.8755555 ... 5.051679  4.639209  4.07304  ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}